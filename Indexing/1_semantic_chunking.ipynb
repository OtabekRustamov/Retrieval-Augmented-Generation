{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Semantic Chunking\n",
    "\n",
    "**What:** Split text where meaning changes, not at fixed sizes\n",
    "\n",
    "**Why:** Preserves sentence integrity and topic flow\n",
    "\n",
    "**When:** Structured documents with clear topic shifts (SOPs, manuals, reports)\n",
    "\n",
    "**How It Works:**\n",
    "1. Split text into sentences\n",
    "2. Compute sentence embeddings\n",
    "3. Measure cosine similarity between adjacent sentences\n",
    "4. Insert split when similarity drops below threshold\n",
    "\n",
    "**Comparison:**\n",
    "```\n",
    "Fixed-size:  \"The cat sat on the | mat. Dogs are loyal ani | mals...\"\n",
    "Semantic:    \"The cat sat on the mat. | Dogs are loyal animals...\"\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `breakpoint_threshold_type`: \"percentile\", \"standard_deviation\", \"interquartile\"\n",
    "- `breakpoint_threshold_amount`: e.g., 95 (percentile), 3 (std dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from config import embeddings, load_documents, format_docs, VECTOR_STORE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Semantic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_chunker(threshold_type=\"percentile\", threshold_amount=95):\n",
    "    \"\"\"Create semantic chunker with specified threshold\"\"\"\n",
    "    return SemanticChunker(\n",
    "        embeddings=embeddings,\n",
    "        breakpoint_threshold_type=threshold_type,\n",
    "        breakpoint_threshold_amount=threshold_amount\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunk_documents(documents, threshold_type=\"percentile\", threshold_amount=95):\n",
    "    \"\"\"Apply semantic chunking to documents\"\"\"\n",
    "    splitter = create_semantic_chunker(threshold_type, threshold_amount)\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Skip very short text\n",
    "        if len(doc.page_content) < 100:\n",
    "            doc.metadata[\"chunk_method\"] = \"original\"\n",
    "            all_chunks.append(doc)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            chunks = splitter.split_documents([doc])\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata.update(doc.metadata)\n",
    "                chunk.metadata[\"chunk_method\"] = \"semantic\"\n",
    "            all_chunks.extend(chunks)\n",
    "        except:\n",
    "            doc.metadata[\"chunk_method\"] = \"original\"\n",
    "            all_chunks.append(doc)\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Fixed-Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_chunking_methods(documents, fixed_size=1000, semantic_threshold=95):\n",
    "    \"\"\"Compare fixed-size vs semantic chunking\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CHUNKING COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Fixed-size chunking\n",
    "    fixed_splitter = RecursiveCharacterTextSplitter(chunk_size=fixed_size, chunk_overlap=200)\n",
    "    fixed_chunks = fixed_splitter.split_documents(documents)\n",
    "    \n",
    "    fixed_lengths = [len(c.page_content) for c in fixed_chunks]\n",
    "    print(f\"\\nFixed-Size Chunks:\")\n",
    "    print(f\"  Count: {len(fixed_chunks)}\")\n",
    "    print(f\"  Avg length: {sum(fixed_lengths)//len(fixed_lengths)} chars\")\n",
    "    \n",
    "    # Semantic chunking\n",
    "    semantic_chunks = semantic_chunk_documents(documents, threshold_amount=semantic_threshold)\n",
    "    \n",
    "    semantic_lengths = [len(c.page_content) for c in semantic_chunks]\n",
    "    print(f\"\\nSemantic Chunks:\")\n",
    "    print(f\"  Count: {len(semantic_chunks)}\")\n",
    "    print(f\"  Avg length: {sum(semantic_lengths)//len(semantic_lengths)} chars\")\n",
    "    \n",
    "    return {\n",
    "        \"fixed_chunks\": fixed_chunks,\n",
    "        \"semantic_chunks\": semantic_chunks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_vectorstore(documents, threshold=95, force_rebuild=False):\n",
    "    \"\"\"Create FAISS vectorstore with semantic chunks\"\"\"\n",
    "    store_path = f\"{VECTOR_STORE_PATH}_semantic\"\n",
    "    \n",
    "    if os.path.exists(store_path) and not force_rebuild:\n",
    "        return FAISS.load_local(store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    chunks = semantic_chunk_documents(documents, threshold_amount=threshold)\n",
    "    print(f\"Created {len(chunks)} semantic chunks\")\n",
    "    \n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore.save_local(store_path)\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "documents = load_documents()\n",
    "\n",
    "# Compare methods\n",
    "comparison = compare_chunking_methods(documents[:10])\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = create_semantic_vectorstore(documents, force_rebuild=True)\n",
    "\n",
    "# Test retrieval\n",
    "test_questions = [\n",
    "    \"What is DeMask?\",\n",
    "    \"What is the difference between Graph DTA and Graph DF?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    docs = vectorstore.similarity_search(q, k=2)\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"  {i+1}. [{doc.metadata.get('filename', 'unknown')}] {doc.page_content[:80]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
