{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-Representation Indexing\n",
    "\n",
    "**What:** Search on short summaries, return full documents\n",
    "\n",
    "**Why:** Summaries are precise for search, full docs provide complete context\n",
    "\n",
    "**When:** Long documents where you need full context for generation\n",
    "\n",
    "**The Problem:**\n",
    "- Hard to find relevant parts in lengthy documents\n",
    "- Chunking loses document-level context\n",
    "- Full doc embedding loses precision\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Full Document (5000+ chars) --> stored in docstore with UUID\n",
    "        |\n",
    "        v LLM generates summary\n",
    "    Summary (200 chars) --> embedded in vectorstore\n",
    "        |\n",
    "        doc_id: links back to full document\n",
    "\n",
    "Query matches Summary --> Returns Full Document\n",
    "```\n",
    "\n",
    "**Variations:**\n",
    "1. Summary-based (this implementation)\n",
    "2. Hypothetical Questions - LLM generates questions the doc answers\n",
    "3. Propositions - Extract key facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from config import embeddings, model, load_documents, format_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Document Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(document):\n",
    "    \"\"\"Generate concise summary of document\"\"\"\n",
    "    template = \"\"\"Summarize in 2-3 sentences. Focus on main topics and key concepts.\n",
    "\n",
    "Document:\n",
    "{content}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    # Truncate if too long\n",
    "    content = document.page_content[:8000]\n",
    "    \n",
    "    return chain.invoke({\"content\": content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Multi-Representation Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_rep_index(documents, max_docs=None):\n",
    "    \"\"\"Create index with summaries linked to full documents\"\"\"\n",
    "    if max_docs:\n",
    "        documents = documents[:max_docs]\n",
    "    \n",
    "    print(f\"Processing {len(documents)} documents...\")\n",
    "    \n",
    "    doc_store = {}\n",
    "    summary_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Store full document\n",
    "        doc_store[doc_id] = doc\n",
    "        \n",
    "        # Generate summary\n",
    "        print(f\"  [{i+1}/{len(documents)}] Summarizing...\")\n",
    "        summary = generate_summary(doc)\n",
    "        \n",
    "        # Create summary document with link to original\n",
    "        summary_doc = Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"source\": doc.metadata.get(\"filename\", \"unknown\")\n",
    "            }\n",
    "        )\n",
    "        summary_docs.append(summary_doc)\n",
    "    \n",
    "    # Create vectorstore from summaries\n",
    "    vectorstore = FAISS.from_documents(summary_docs, embeddings)\n",
    "    \n",
    "    print(f\"Indexed {len(summary_docs)} summaries\")\n",
    "    \n",
    "    return vectorstore, doc_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Full Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_multi_rep(question, vectorstore, doc_store, k=3):\n",
    "    \"\"\"Search summaries, return full documents\"\"\"\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    # Search summaries\n",
    "    summaries = vectorstore.similarity_search(question, k=k)\n",
    "    print(f\"Matched {len(summaries)} summaries\")\n",
    "    \n",
    "    # Get full documents\n",
    "    full_docs = []\n",
    "    for summary in summaries:\n",
    "        doc_id = summary.metadata.get(\"doc_id\")\n",
    "        if doc_id and doc_id in doc_store:\n",
    "            full_docs.append(doc_store[doc_id])\n",
    "            print(f\"  Summary: {summary.page_content[:80]}...\")\n",
    "    \n",
    "    print(f\"\\nReturning {len(full_docs)} full documents\")\n",
    "    return full_docs, summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Multi-Rep RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_rep_rag(question, vectorstore, doc_store, k=3):\n",
    "    \"\"\"Complete RAG with multi-representation retrieval\"\"\"\n",
    "    full_docs, summaries = retrieve_multi_rep(question, vectorstore, doc_store, k)\n",
    "    \n",
    "    context = format_docs(full_docs)\n",
    "    \n",
    "    template = \"\"\"Answer based on context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    return answer, full_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "documents = load_documents()\n",
    "\n",
    "# Create index (limit docs for speed)\n",
    "vectorstore, doc_store = create_multi_rep_index(documents, max_docs=5)\n",
    "\n",
    "# Test queries\n",
    "test_questions = [\n",
    "    \"What is RF and RDKit?\",\n",
    "    \"What is LibINVENT?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(\"=\"*60)\n",
    "    answer, docs = multi_rep_rag(q, vectorstore, doc_store)\n",
    "    print(f\"\\nSources: {len(docs)} full documents\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"  {i+1}. {doc.metadata.get('filename', 'unknown')} ({len(doc.page_content)} chars)\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
