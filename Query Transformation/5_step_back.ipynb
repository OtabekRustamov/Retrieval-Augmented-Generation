{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Step-Back Prompting\n",
    "\n",
    "**What:** Generate a broader question first to get background context\n",
    "\n",
    "**Why:** Combines general principles with specific details\n",
    "\n",
    "**When:** Technical/conceptual questions that need background\n",
    "\n",
    "**Key Idea:** \"Zoom out before zooming in\"\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Specific Question → General Step-Back Question\n",
    "        ↓                      ↓\n",
    "  Specific Docs          General Docs\n",
    "        ↓                      ↓\n",
    "         Combine → Final Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from config import model, setup_vectorstore, get_retriever, format_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Step-Back Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back(question):\n",
    "    template = \"\"\"Generate a broader, more general version of this question.\n",
    "Ask about principles, concepts, or background knowledge.\n",
    "\n",
    "Specific: {question}\n",
    "\n",
    "General (ONE question only):\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    return chain.invoke({\"question\": question}).strip()\n",
    "\n",
    "# Test\n",
    "q = \"What optimizer did the RL model use?\"\n",
    "print(f\"Specific: {q}\")\n",
    "print(f\"Step-back: {generate_step_back(q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-Back Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_back_retrieve(question, retriever, k=3):\n",
    "    print(f\"Specific: {question}\\n\")\n",
    "    \n",
    "    step_back_q = generate_step_back(question)\n",
    "    print(f\"Step-Back: {step_back_q}\\n\")\n",
    "    \n",
    "    general_docs = retriever.invoke(step_back_q)[:k]\n",
    "    specific_docs = retriever.invoke(question)[:k]\n",
    "    \n",
    "    print(f\"Retrieved: {len(general_docs)} general + {len(specific_docs)} specific\")\n",
    "    \n",
    "    return general_docs, specific_docs, step_back_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Step-Back RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_back_rag(question, retriever):\n",
    "    general_docs, specific_docs, step_back_q = step_back_retrieve(question, retriever)\n",
    "    \n",
    "    general_context = format_docs(general_docs)\n",
    "    specific_context = format_docs(specific_docs)\n",
    "    \n",
    "    template = \"\"\"Answer using both general principles and specific details.\n",
    "\n",
    "GENERAL (principles):\n",
    "{general_context}\n",
    "\n",
    "SPECIFIC (details):\n",
    "{specific_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    answer = chain.invoke({\n",
    "        \"general_context\": general_context,\n",
    "        \"specific_context\": specific_context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = setup_vectorstore()\n",
    "retriever = get_retriever(vectorstore, k=5)\n",
    "\n",
    "test_questions = [\n",
    "    \"What role does DeMask play?\",\n",
    "    \"What is the difference between Graph DTA and Graph DF?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(\"=\"*60)\n",
    "    step_back_rag(q, retriever)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
