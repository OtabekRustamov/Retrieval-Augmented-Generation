{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multi-Query Generation\n",
    "\n",
    "**What:** Generate 3-5 variations of the original question\n",
    "\n",
    "**Why:** Different wordings catch different relevant documents\n",
    "\n",
    "**When:** Ambiguous queries, need better recall\n",
    "\n",
    "**Stage:** Pre-Retrieval\n",
    "\n",
    "**LLM Calls:** 2 (generate variations + final answer)\n",
    "\n",
    "---\n",
    "\n",
    "### Flow\n",
    "\n",
    "```\n",
    "Original Query → LLM → [Query 1, Query 2, Query 3, ...]\n",
    "                              ↓\n",
    "                     Retrieve for each query\n",
    "                              ↓\n",
    "                     Combine unique documents\n",
    "                              ↓\n",
    "                        Generate Answer\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "**Original:** \"Where did Otabek study?\"\n",
    "\n",
    "**Variations:**\n",
    "- \"What university did Otabek attend?\"\n",
    "- \"Where did Otabek receive his education?\"\n",
    "- \"What is Otabek's educational institution?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from config import model, setup_vectorstore, get_retriever, format_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Multiple Query Variations\n",
    "\n",
    "Generate different versions of the question to improve retrieval coverage.\n",
    "\n",
    "**Args:**\n",
    "- `question`: Original user question\n",
    "- `num_variations`: Number of variations to generate (default: 3)\n",
    "\n",
    "**Returns:**\n",
    "- List of queries including the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_queries(question, num_variations=3):\n",
    "    \"\"\"Generate multiple query variations\"\"\"\n",
    "    \n",
    "    template = \"\"\"You are an AI assistant that generates alternative versions of questions\n",
    "to improve document retrieval in a RAG system.\n",
    "\n",
    "Generate {num_variations} different versions of the given question.\n",
    "Each version should ask the same thing but with different words/perspectives.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Provide numbered alternatives:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"num_variations\": num_variations\n",
    "    })\n",
    "    \n",
    "    # Parse response\n",
    "    queries = [question]  # Always include original\n",
    "    \n",
    "    for line in response.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line and len(line) > 5:\n",
    "            # Remove numbering\n",
    "            if line[0].isdigit():\n",
    "                line = line.split(\". \", 1)[-1].split(\") \", 1)[-1]\n",
    "            queries.append(line)\n",
    "    \n",
    "    return queries[:num_variations + 1]\n",
    "\n",
    "\n",
    "# Test\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "queries = generate_multi_queries(question, 5)\n",
    "\n",
    "print(\"Generated Queries:\")\n",
    "for i, q in enumerate(queries):\n",
    "    prefix = \"(original)\" if i == 0 else f\"{i}.\"\n",
    "    print(f\"  {prefix} {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Multi-Query Retrieval\n",
    "\n",
    "Retrieve documents for each query variation and combine unique results.\n",
    "\n",
    "**Process:**\n",
    "1. Generate query variations\n",
    "2. Retrieve for each query\n",
    "3. Deduplicate by content fingerprint\n",
    "\n",
    "**Args:**\n",
    "- `question`: Original user question\n",
    "- `retriever`: LangChain retriever\n",
    "- `num_variations`: Number of variations\n",
    "\n",
    "**Returns:**\n",
    "- List of unique documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_retrieve(question, retriever, num_variations=3):\n",
    "    \"\"\"Retrieve using multiple query variations\"\"\"\n",
    "    \n",
    "    # Generate variations\n",
    "    queries = generate_multi_queries(question, num_variations)\n",
    "    \n",
    "    print(\"Generated Queries:\")\n",
    "    for i, q in enumerate(queries):\n",
    "        prefix = \"(original)\" if i == 0 else f\"{i}.\"\n",
    "        print(f\"  {prefix} {q}\")\n",
    "    \n",
    "    # Retrieve for each query\n",
    "    all_docs = []\n",
    "    seen_content = set()\n",
    "    \n",
    "    for query in queries:\n",
    "        docs = retriever.invoke(query)\n",
    "        for doc in docs:\n",
    "            # Deduplicate by first 100 chars\n",
    "            fingerprint = doc.page_content[:100]\n",
    "            if fingerprint not in seen_content:\n",
    "                seen_content.add(fingerprint)\n",
    "                all_docs.append(doc)\n",
    "    \n",
    "    print(f\"\\nRetrieved {len(all_docs)} unique documents\")\n",
    "    for i, doc in enumerate(all_docs[:5]):\n",
    "        source = doc.metadata.get('filename', 'unknown')\n",
    "        print(f\"  {i+1}. [{source}] {doc.page_content[:60]}...\")\n",
    "    \n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Complete Multi-Query RAG Pipeline\n",
    "\n",
    "Full pipeline: Generate variations → Retrieve → Generate answer\n",
    "\n",
    "**Args:**\n",
    "- `question`: User question\n",
    "- `retriever`: LangChain retriever\n",
    "- `num_variations`: Number of variations\n",
    "\n",
    "**Returns:**\n",
    "- Final answer string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_rag(question, retriever, num_variations=3):\n",
    "    \"\"\"Complete Multi-Query RAG pipeline\"\"\"\n",
    "    \n",
    "    # Retrieve\n",
    "    docs = multi_query_retrieve(question, retriever, num_variations)\n",
    "    context = format_docs(docs[:5])\n",
    "    \n",
    "    # Generate answer\n",
    "    answer_template = \"\"\"Answer the question based ONLY on the provided context.\n",
    "If the context doesn't contain the answer, say \"Information not found in documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(answer_template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"=\"*70)\n",
    "    print(answer)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "vectorstore = setup_vectorstore()\n",
    "retriever = get_retriever(vectorstore, k=3)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"Where did Otabek study?\",\n",
    "    \"How to use DeMask?\",\n",
    "    \"What is DMS?\",\n",
    "    \"What is the difference between Graph DTA and Graph DF?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\"*70)\n",
    "    answer = multi_query_rag(question, retriever)\n",
    "    print(\"\\n\" + \"-\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
