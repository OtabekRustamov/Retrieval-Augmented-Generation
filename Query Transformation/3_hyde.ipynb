{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "**What:** Generate hypothetical answer, embed it, retrieve real docs\n",
    "\n",
    "**Why:** Hypothetical answers are closer in embedding space to real answers\n",
    "\n",
    "**When:** Technical/semantic queries where keyword search fails\n",
    "\n",
    "**Key Insight:**\n",
    "- Question → Real Answer: ~0.65 similarity\n",
    "- Hypothetical → Real Answer: ~0.89 similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from config import model, embeddings, setup_vectorstore, format_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hypothetical Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical(question):\n",
    "    template = \"\"\"Write a short paragraph answering this question.\n",
    "Even if unsure, write what a good answer might look like.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    return chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyDE Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyde_retrieve(question, vectorstore, k=5):\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    hypothetical = generate_hypothetical(question)\n",
    "    print(f\"Hypothetical: {hypothetical[:150]}...\\n\")\n",
    "    \n",
    "    docs = vectorstore.similarity_search(hypothetical, k=k)\n",
    "    \n",
    "    print(f\"Retrieved {len(docs)} documents\")\n",
    "    return docs, hypothetical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete HyDE RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyde_rag(question, vectorstore, k=5):\n",
    "    docs, hypothetical = hyde_retrieve(question, vectorstore, k)\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    template = \"\"\"Answer based on context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = setup_vectorstore()\n",
    "\n",
    "test_questions = [\n",
    "    \"Where did Otabek study?\",\n",
    "    \"What is DMS?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(\"=\"*60)\n",
    "    hyde_rag(q, vectorstore)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
