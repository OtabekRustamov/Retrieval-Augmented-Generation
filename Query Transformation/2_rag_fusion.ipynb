{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RAG-Fusion (with Reciprocal Rank Fusion)\n",
    "\n",
    "**What:** Multi-Query + intelligent ranking using RRF\n",
    "\n",
    "**Why:** Documents appearing highly across multiple queries get priority\n",
    "\n",
    "**When:** When ranking quality is critical (biomedical, legal, scientific QA)\n",
    "\n",
    "**Stage:** Pre-Retrieval (query generation) + Post-Retrieval (re-ranking)\n",
    "\n",
    "**LLM Calls:** 2 (generate variations + final answer)\n",
    "\n",
    "---\n",
    "\n",
    "### RRF Formula\n",
    "\n",
    "```\n",
    "RRF_score = Σ[1 / (k + rank)]  where k=60\n",
    "```\n",
    "\n",
    "### Why RRF is Better Than Score Averaging\n",
    "\n",
    "- Uses **rank position**, not absolute similarity scores\n",
    "- Robust to **different scoring scales** (e.g., cosine 0-1 vs BM25 0-100)\n",
    "- Boosts documents that rank **high across multiple queries**\n",
    "\n",
    "### Example\n",
    "\n",
    "A document appears at ranks [0, 2, 0] across 3 queries:\n",
    "\n",
    "```\n",
    "RRF = 1/(60+0) + 1/(60+2) + 1/(60+0)\n",
    "    = 0.0167 + 0.0161 + 0.0167\n",
    "    = 0.0495\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from config import model, setup_vectorstore, get_retriever, format_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Query Variations\n",
    "\n",
    "Same as Multi-Query - generate different versions of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_queries(question, num_variations=3):\n",
    "    \"\"\"Generate multiple query variations\"\"\"\n",
    "    \n",
    "    template = \"\"\"Generate {num_variations} different versions of the given question.\n",
    "Each version should ask the same thing but with different words/perspectives.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Provide numbered alternatives:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"num_variations\": num_variations\n",
    "    })\n",
    "    \n",
    "    queries = [question]\n",
    "    for line in response.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line and len(line) > 5:\n",
    "            if line[0].isdigit():\n",
    "                line = line.split(\". \", 1)[-1].split(\") \", 1)[-1]\n",
    "            queries.append(line)\n",
    "    \n",
    "    return queries[:num_variations + 1]\n",
    "\n",
    "\n",
    "# Test\n",
    "queries = generate_multi_queries(\"What is task decomposition?\", 3)\n",
    "for q in queries:\n",
    "    print(f\"  - {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "Combine multiple result lists with rank-based scoring.\n",
    "\n",
    "**Args:**\n",
    "- `results_list`: List of document lists from different queries\n",
    "- `k`: RRF constant (default=60)\n",
    "\n",
    "**Returns:**\n",
    "- List of (document, rrf_score) tuples, sorted by score descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results_list, k=60):\n",
    "    \"\"\"Combine results using RRF scoring\"\"\"\n",
    "    \n",
    "    fused_scores = {}\n",
    "    doc_map = {}\n",
    "    \n",
    "    for docs in results_list:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Use first 100 chars as document ID\n",
    "            doc_id = doc.page_content[:100]\n",
    "            \n",
    "            if doc_id not in fused_scores:\n",
    "                fused_scores[doc_id] = 0\n",
    "                doc_map[doc_id] = doc\n",
    "            \n",
    "            # RRF formula\n",
    "            fused_scores[doc_id] += 1 / (k + rank)\n",
    "    \n",
    "    # Sort by score descending\n",
    "    sorted_docs = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [(doc_map[doc_id], score) for doc_id, score in sorted_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: RAG-Fusion Retrieval\n",
    "\n",
    "Generate queries → Retrieve for each → Apply RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_fusion_retrieve(question, retriever, num_variations=3):\n",
    "    \"\"\"RAG-Fusion retrieval with RRF\"\"\"\n",
    "    \n",
    "    # Generate queries\n",
    "    queries = generate_multi_queries(question, num_variations)\n",
    "    \n",
    "    print(\"Generated Queries:\")\n",
    "    for i, q in enumerate(queries):\n",
    "        prefix = \"(original)\" if i == 0 else f\"{i}.\"\n",
    "        print(f\"  {prefix} {q}\")\n",
    "    \n",
    "    # Retrieve for each query\n",
    "    results_list = [retriever.invoke(q) for q in queries]\n",
    "    \n",
    "    # Apply RRF\n",
    "    ranked = reciprocal_rank_fusion(results_list)\n",
    "    \n",
    "    print(f\"\\nRRF Ranked Results (top 5):\")\n",
    "    for i, (doc, score) in enumerate(ranked[:5]):\n",
    "        source = doc.metadata.get('filename', 'unknown')\n",
    "        print(f\"  {i+1}. Score: {score:.4f} | {source}\")\n",
    "        print(f\"     Preview: {doc.page_content[:60]}...\")\n",
    "    \n",
    "    return ranked, queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Complete RAG-Fusion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_fusion_rag(question, retriever, num_variations=3, top_k=5):\n",
    "    \"\"\"Complete RAG-Fusion pipeline\"\"\"\n",
    "    \n",
    "    # Retrieve with RRF\n",
    "    ranked, queries = rag_fusion_retrieve(question, retriever, num_variations)\n",
    "    \n",
    "    # Get top documents\n",
    "    docs = [doc for doc, _ in ranked[:top_k]]\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer_template = \"\"\"Answer based ONLY on the context.\n",
    "If not found, say \"Information not found in documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(answer_template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"=\"*70)\n",
    "    print(answer)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "vectorstore = setup_vectorstore()\n",
    "retriever = get_retriever(vectorstore, k=5)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"Where did Otabek study?\",\n",
    "    \"How to use DeMask?\",\n",
    "    \"What is DMS?\",\n",
    "    \"What is the difference between Graph DTA and Graph DF?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\"*70)\n",
    "    answer = rag_fusion_rag(question, retriever)\n",
    "    print(\"\\n\" + \"-\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
