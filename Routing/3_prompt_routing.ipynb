{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66e8c11-9dec-4e06-9473-86af39b0c036",
   "metadata": {},
   "source": [
    "## PATTERN 3: PROMPT TEMPLATE ROUTING\n",
    "\n",
    "### Routes queries to different **prompt templates** based on the desired response style:\n",
    "\n",
    "- **technical**: Coding, engineering, APIs  \n",
    "- **creative**: Writing, storytelling, brainstorming  \n",
    "- **educational**: Learning, tutorials, explanations  \n",
    "- **analytical**: Comparisons, analysis, decision-making  \n",
    "- **conversational**: Casual chat, simple Q&A  \n",
    "\n",
    "```\n",
    "Diagram:\n",
    "    Query → Router → Prompt 1 (Technical) ─┐\n",
    "                  → Prompt 2 (Creative)  ──┼── → LLM → Response\n",
    "                  → Prompt 3 (Educational)─┘\n",
    "```\n",
    "\n",
    "\n",
    "All routes go to the **same LLM**, but use **different system prompts**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2070ad-b921-4af7-a459-c0ae920b828b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OPENAI_API_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Import from config\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m llm, PROMPT_TEMPLATES\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LLM\\RAG\\Routing\\config.py:56\u001b[39m\n\u001b[32m     46\u001b[39m DOCUMENT_MAPPING = {\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcv_resume\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCV.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdms_info\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDMS.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mllm_interview\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLLM_Interview.pdf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m }\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# INITIALIZE LLM & EMBEDDINGS\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     55\u001b[39m llm = ChatOpenAI(\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     api_key=\u001b[43mOPENAI_API_KEY\u001b[49m,\n\u001b[32m     57\u001b[39m     model=LLM_MODEL,\n\u001b[32m     58\u001b[39m     temperature=TEMPERATURE\n\u001b[32m     59\u001b[39m )\n\u001b[32m     61\u001b[39m embeddings = OpenAIEmbeddings(\n\u001b[32m     62\u001b[39m     api_key=OPENAI_API_KEY,\n\u001b[32m     63\u001b[39m     model=EMBEDDING_MODEL\n\u001b[32m     64\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# DOCUMENT LOADING FUNCTIONS\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'OPENAI_API_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Import from config\n",
    "from config import llm, PROMPT_TEMPLATES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2376997e-2a2c-42c5-adaa-5703b9f92789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptRoute(BaseModel):\n",
    "    \"\"\"Structured output for prompt routing.\"\"\"\n",
    "    prompt_type: Literal[\"technical\", \"creative\", \"educational\", \"analytical\", \"conversational\"] = Field(\n",
    "        description=\"The prompt template style to use\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"Why this prompt style was chosen\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30b935-a60a-43c3-ac16-4922f3c498f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_router():\n",
    "    \"\"\"Create the prompt template router.\"\"\"\n",
    "    system = \"\"\"You are an expert at choosing the right response style.\n",
    "\n",
    "PROMPT TEMPLATES:\n",
    "- technical: Coding, engineering, APIs, debugging, system design\n",
    "- creative: Stories, poems, marketing copy, brainstorming, naming ideas\n",
    "- educational: \"Explain X\", \"How does Y work?\", teaching, tutorials\n",
    "- analytical: \"Compare A vs B\", analysis, pros/cons, decision-making\n",
    "- conversational: Casual chat, simple Q&A, greetings, personal advice\n",
    "\n",
    "ROUTING RULES:\n",
    "- \"How do I implement...\" → technical\n",
    "- \"Debug this code...\" → technical\n",
    "- \"Write a story/poem about...\" → creative\n",
    "- \"Give me creative names for...\" → creative\n",
    "- \"Explain X in simple terms...\" → educational\n",
    "- \"Teach me about...\" → educational\n",
    "- \"Compare X and Y...\" → analytical\n",
    "- \"What are the pros and cons...\" → analytical\n",
    "- \"Hey, how are you?\" → conversational\n",
    "- \"Thanks!\" → conversational\n",
    "\n",
    "Choose the template that matches the query's desired response style.\"\"\"\n",
    "\n",
    "    router_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(PromptRoute)\n",
    "    return router_prompt | structured_llm\n",
    "\n",
    "\n",
    "router = create_router()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c903bef-2633-42e4-bd91-bb5bee1de00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(question: str):\n",
    "    \"\"\"\n",
    "    Route and answer a question using prompt template routing.\n",
    "    \n",
    "    Args:\n",
    "        question: User question\n",
    "    \n",
    "    Returns:\n",
    "        Dict with prompt_type, confidence, reasoning, answer\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" QUESTION: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Step 1: Route the query\n",
    "    route = router.invoke({\"question\": question})\n",
    "    print(f\" Routing to: {route.prompt_type} prompt (confidence: {route.confidence:.2f})\")\n",
    "    print(f\"   Reasoning: {route.reasoning}\")\n",
    "    \n",
    "    # Step 2: Get the appropriate prompt template (IF-ELSE LOGIC)\n",
    "    if route.prompt_type in PROMPT_TEMPLATES:\n",
    "        system_prompt = PROMPT_TEMPLATES[route.prompt_type]\n",
    "    else:\n",
    "        system_prompt = PROMPT_TEMPLATES[\"conversational\"]\n",
    "    \n",
    "    # Step 3: Generate response with selected prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"question\": question})\n",
    "    \n",
    "    print(f\"\\n ANSWER ({route.prompt_type} style):\\n{'-'*80}\\n{answer}\\n{'-'*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"prompt_type\": route.prompt_type,\n",
    "        \"confidence\": route.confidence,\n",
    "        \"reasoning\": route.reasoning,\n",
    "        \"answer\": answer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef5afb-0e57-4064-8bd8-09c55ee798ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_prompts(question: str):\n",
    "    \"\"\"\n",
    "    Generate responses using ALL prompt templates for comparison.\n",
    "    Useful for demonstrating how different prompts affect output.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" COMPARING ALL PROMPTS: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for prompt_type, system_prompt in PROMPT_TEMPLATES.items():\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        answer = chain.invoke({\"question\": question})\n",
    "        \n",
    "        results[prompt_type] = answer\n",
    "        \n",
    "        print(f\"\\n {prompt_type.upper()}:\\n{'-'*40}\")\n",
    "        print(answer[:300] + \"...\" if len(answer) > 300 else answer)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb51f7-6de7-445a-bd0d-6ae49c0ecf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_questions = [\n",
    "        (\"How do I implement a REST API in Python?\", \"technical\"),\n",
    "        (\"Write a short poem about coding\", \"creative\"),\n",
    "        (\"Explain neural networks to a beginner\", \"educational\"),\n",
    "        (\"Compare PostgreSQL vs MongoDB\", \"analytical\"),\n",
    "        (\"Hey, how are you today?\", \"conversational\"),\n",
    "    ]\n",
    "    \n",
    "for question, expected in test_questions:\n",
    "    result = query_rag(question)\n",
    "    match = \"OK\" if result[\"prompt_type\"] == expected else \"X\"\n",
    "        print(f\"{match} Expected: {expected} | Got: {result['prompt_type']}\\n\")\n",
    "    \n",
    "# Demo: Compare same question with different prompts\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEMO: Same question, different prompt styles\")\n",
    "print(\"=\" * 80)\n",
    "compare_all_prompts(\"What is machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b36b112-c457-4ad0-a3a8-f02c17f75b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
